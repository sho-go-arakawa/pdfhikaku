import os
import streamlit as st
from typing import List, Dict, Any
import openai
import json
from utils.pinecone_io import PineconeManager


# è¿½åŠ : explanation æ­£è¦åŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼
def _normalize_explanation(value):
    """
    explanation ã‚’ UI è¡¨ç¤ºç”¨ã®æ–‡å­—åˆ—ã«æ­£è¦åŒ–ã™ã‚‹ã€‚
    - list ã®å ´åˆ: ç®‡æ¡æ›¸ãã® Markdown ã¸å¤‰æ›
    - str ã®å ´åˆ: ãã®ã¾ã¾è¿”ã™
    - ãã®ä»–/None: ç©ºæ–‡å­—ã‚’è¿”ã™
    """
    if isinstance(value, list):
        items = [str(x).strip() for x in value if str(x).strip()]
        if not items:
            return ""
        return "\n".join(f"- {x}" for x in items)
    if isinstance(value, str):
        return value
    return ""


class RAGChatbot:
    def __init__(self):
        self.client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
        self.embedding_model = "text-embedding-3-small"
        self.chat_model = "gpt-3.5-turbo"
        self.pinecone_manager = PineconeManager()
        
        self.system_prompt = """ã‚ãªãŸã¯ä¼æ¥­å†…è¦å®šã«é–¢ã™ã‚‹QAã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæ–‡è„ˆï¼ˆretrieved_contextï¼‰ã®æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³å®ˆã—ã¦ã€**å¿…ãšJSONã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›æ§‹é€ ã€‘

{
  "answer": "è³ªå•ã«å¯¾ã™ã‚‹ç°¡æ½”ãªçµè«–ï¼ˆ1ï½3æ–‡ï¼‰",
  "explanation": "èƒŒæ™¯ã‚„æ³¨æ„ç‚¹ã‚’å«ã‚€è§£èª¬ï¼ˆ5ï½8æ–‡ã€å°‚é–€ç”¨èªã¯ã‚„ã•ã—ãï¼‰",
  "highlights": [
    {
      "source_id": "company_policies",
      "chunk_index": 12,
      "span": [128, 215],
      "quote": "å¾“æ¥­å“¡ã¯...ã‚ã‚‰ã‹ã˜ã‚ç”³è«‹ãŒå¿…è¦ã§ã™ã€‚"
    }
  ],
  "sources": [
    {
      "source_id": "company_policies",
      "chunk_index": 12,
      "confidence": "High"
    }
  ]
}

ã€ãƒ«ãƒ¼ãƒ«ã€‘

1. answer ã¯äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
2. explanation ã¯è¦ç‚¹ãƒ»èƒŒæ™¯ãƒ»æ³¨æ„ç‚¹ã‚’å«ã‚ã€èª­ã¿æ‰‹ãŒç†è§£ã—ã‚„ã™ã„æ§‹æˆã«ã—ã¦ãã ã•ã„ï¼ˆç®‡æ¡æ›¸ãå¯ï¼‰ã€‚
3. highlights ã¯åŸæ–‡ã®é‡è¦ãªéƒ¨åˆ†ã‚’ãã®ã¾ã¾çŸ­ãå¼•ç”¨ã—ã€ãƒãƒ£ãƒ³ã‚¯å†…ã®æ–‡å­—ç¯„å›² [start, end] ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§5ä»¶ï¼‰ã€‚
4. sources ã¯å›ç­”ã«ä½¿ã£ãŸãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã¾ã¨ã‚ã€confidence ã‚’ "High" / "Med" / "Low" ã®ã„ãšã‚Œã‹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
5. ä¸æ˜ãªå ´åˆã¯ "ä¸æ˜ã§ã™" ã¨ç­”ãˆã€è¿½åŠ ã§å¿…è¦ãªæƒ…å ±ã‚’1ã€œ2ç‚¹æç¤ºã—ã¦ãã ã•ã„ã€‚
6. æ¨æ¸¬ã™ã‚‹å ´åˆã¯ "æ¨æ¸¬: ..." ã¨æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚
7. JSON ä»¥å¤–ã®æ–‡å­—åˆ—ï¼ˆèª¬æ˜æ–‡ã‚„æ³¨é‡ˆï¼‰ã¯çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚"""
    
    def create_query_embedding(self, query: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        response = self.client.embeddings.create(
            model=self.embedding_model,
            input=query
        )
        return response.data[0].embedding
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢"""
        query_vector = self.create_query_embedding(query)
        return self.pinecone_manager.search_similar(query_vector, top_k=top_k)
    
    def generate_response(self, query: str, context_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ç”Ÿæˆ"""
        # retrieved_context ã‚’æ§‹ç¯‰
        retrieved_context = []
        for i, chunk in enumerate(context_chunks):
            retrieved_context.append({
                "source_id": "company_policies",
                "chunk_index": chunk["metadata"]["chunk_index"],
                "text": chunk["text"],
                "char_start": chunk["metadata"]["char_start"],
                "char_end": chunk["metadata"]["char_end"]
            })
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        user_message = {
            "question": query,
            "retrieved_context": retrieved_context
        }
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": json.dumps(user_message, ensure_ascii=False, indent=2)}
        ]
        
        response = self.client.chat.completions.create(
            model=self.chat_model,
            messages=messages,
            temperature=0.1,
            max_tokens=800
        )
        
        # JSON ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        try:
            response_text = response.choices[0].message.content.strip()
            # JSONãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã¯é™¤å»
            if response_text.startswith('```json'):
                response_text = response_text[7:-3].strip()
            elif response_text.startswith('```'):
                response_text = response_text[3:-3].strip()
            
            parsed_response = json.loads(response_text)
            return parsed_response
        except json.JSONDecodeError as e:
            # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "explanation": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}",
                "highlights": [],
                "sources": []
            }


def main():
    st.set_page_config(
        page_title="ç¤¾å†…è¦å®šãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        page_icon="ğŸ“˜",
        layout="wide"
    )
    
    st.title("ğŸ“˜ ç¤¾å†…è¦å®šãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    st.markdown("ç¤¾å†…ã®è¦å®šã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
    with st.sidebar:
        st.header("è¨­å®š")
        top_k = st.slider("æ¤œç´¢çµæœæ•°", min_value=1, max_value=10, value=5)
        show_context = st.checkbox("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º", value=False)
        show_highlights = st.checkbox("ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’è¡¨ç¤º", value=True)
        show_sources = st.checkbox("å‡ºå…¸ã‚’è¡¨ç¤º", value=True)
        
        st.header("ä½¿ç”¨æ–¹æ³•")
        st.markdown("""
        1. ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚’å…¥åŠ›
        2. Enterã‚­ãƒ¼ã¾ãŸã¯ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        3. AI ãŒç¤¾å†…è¦å®šã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™
        
        **ä¾‹ï¼š**
        - ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ ã®ã‚³ã‚¢ã‚¿ã‚¤ãƒ ã¯ä½•æ™‚ã§ã™ã‹ï¼Ÿ
        - æœ‰çµ¦ä¼‘æš‡ã®å–å¾—æ–¹æ³•ã‚’æ•™ãˆã¦
        - ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®è¦å®šã«ã¤ã„ã¦
        """)
    
    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®åˆæœŸåŒ–
    if "chatbot" not in st.session_state:
        with st.spinner("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’åˆæœŸåŒ–ä¸­..."):
            try:
                st.session_state.chatbot = RAGChatbot()
                st.success("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            except Exception as e:
                st.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                st.stop()
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                st.markdown(message["content"])
            else:
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¤º
                response_data = message.get("response_data")
                if response_data and isinstance(response_data, dict):
                    # å›ç­”ã‚’è¡¨ç¤º
                    st.markdown(f"**ğŸ“ å›ç­”:**")
                    st.markdown(response_data.get("answer", ""))
                    
                    # è§£èª¬ã‚’è¡¨ç¤ºï¼ˆé…åˆ—â†’Markdownæ–‡å­—åˆ—ã«æ­£è¦åŒ–ï¼‰
                    raw_explanation = response_data.get("explanation", "")
                    normalized_explanation = _normalize_explanation(raw_explanation)
                    if normalized_explanation:
                        st.markdown("**ğŸ’¡ è§£èª¬:**")
                        st.markdown(normalized_explanation)
                    
                    # ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’è¡¨ç¤º
                    if show_highlights and response_data.get("highlights"):
                        with st.expander("ğŸ” å¼•ç”¨ãƒã‚¤ãƒ©ã‚¤ãƒˆ"):
                            for i, highlight in enumerate(response_data.get("highlights", [])):
                                st.markdown(f"**å¼•ç”¨ {i+1}:**")
                                st.code(highlight.get("quote", ""), language=None)
                                st.caption(f"ãƒãƒ£ãƒ³ã‚¯ {highlight.get('chunk_index', 'N/A')}, ä½ç½®: {highlight.get('span', [])}")
                    
                    # å‡ºå…¸ã‚’è¡¨ç¤º  
                    if show_sources and response_data.get("sources"):
                        with st.expander("ğŸ“š å‡ºå…¸æƒ…å ±"):
                            for source in response_data.get("sources", []):
                                confidence_color = {
                                    "High": "ğŸŸ¢", 
                                    "Med": "ğŸŸ¡", 
                                    "Low": "ğŸ”´"
                                }.get(source.get("confidence", "Med"), "âšª")
                                
                                st.markdown(f"{confidence_color} ãƒãƒ£ãƒ³ã‚¯ {source.get('chunk_index', 'N/A')} - ä¿¡é ¼åº¦: {source.get('confidence', 'Med')}")
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥å½¢å¼
                    st.markdown(message["content"])
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºï¼ˆå¾“æ¥é€šã‚Šï¼‰
                if show_context and "context" in message:
                    with st.expander("å‚è€ƒã«ã—ãŸç¤¾å†…è¦å®šã®æŠœç²‹"):
                        for i, chunk in enumerate(message["context"]):
                            st.markdown(f"**[é–¢é€£æƒ…å ± {i+1}] (é¡ä¼¼åº¦: {chunk['score']:.3f})**")
                            st.markdown(chunk["text"])
                            st.markdown("---")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    if prompt := st.chat_input("ç¤¾å†…è¦å®šã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                try:
                    # é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢
                    context_chunks = st.session_state.chatbot.search_relevant_chunks(
                        prompt, top_k=top_k
                    )
                    
                    if not context_chunks:
                        response_data = {
                            "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãŠæ¢ã—ã®æƒ…å ±ãŒç¤¾å†…è¦å®šã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                            "explanation": "æ¤œç´¢çµæœã«è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¡¨ç¾ã§è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
                            "highlights": [],
                            "sources": []
                        }
                        context_chunks = []
                    else:
                        # å›ç­”ã‚’ç”Ÿæˆ
                        response_data = st.session_state.chatbot.generate_response(
                            prompt, context_chunks
                        )
                    
                    # æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’è¡¨ç¤º
                    st.markdown(f"**ğŸ“ å›ç­”:**")
                    st.markdown(response_data.get("answer", ""))
                    
                    # è§£èª¬ã‚’è¡¨ç¤ºï¼ˆé…åˆ—â†’Markdownæ–‡å­—åˆ—ã«æ­£è¦åŒ–ï¼‰
                    raw_explanation = response_data.get("explanation", "")
                    normalized_explanation = _normalize_explanation(raw_explanation)
                    if normalized_explanation:
                        st.markdown("**ğŸ’¡ è§£èª¬:**")
                        st.markdown(normalized_explanation)
                    
                    # ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’è¡¨ç¤º
                    if show_highlights and response_data.get("highlights"):
                        with st.expander("ğŸ” å¼•ç”¨ãƒã‚¤ãƒ©ã‚¤ãƒˆ"):
                            for i, highlight in enumerate(response_data.get("highlights", [])):
                                st.markdown(f"**å¼•ç”¨ {i+1}:**")
                                st.code(highlight.get("quote", ""), language=None)
                                st.caption(f"ãƒãƒ£ãƒ³ã‚¯ {highlight.get('chunk_index', 'N/A')}, ä½ç½®: {highlight.get('span', [])}")
                    
                    # å‡ºå…¸ã‚’è¡¨ç¤º  
                    if show_sources and response_data.get("sources"):
                        with st.expander("ğŸ“š å‡ºå…¸æƒ…å ±"):
                            for source in response_data.get("sources", []):
                                confidence_color = {
                                    "High": "ğŸŸ¢", 
                                    "Med": "ğŸŸ¡", 
                                    "Low": "ğŸ”´"
                                }.get(source.get("confidence", "Med"), "âšª")
                                
                                st.markdown(f"{confidence_color} ãƒãƒ£ãƒ³ã‚¯ {source.get('chunk_index', 'N/A')} - ä¿¡é ¼åº¦: {source.get('confidence', 'Med')}")
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºï¼ˆå¾“æ¥é€šã‚Šï¼‰
                    if context_chunks and show_context:
                        with st.expander("å‚è€ƒã«ã—ãŸç¤¾å†…è¦å®šã®æŠœç²‹"):
                            for i, chunk in enumerate(context_chunks):
                                st.markdown(f"**[é–¢é€£æƒ…å ± {i+1}] (é¡ä¼¼åº¦: {chunk['score']:.3f})**")
                                st.markdown(chunk["text"])
                                st.markdown("---")
                    
                    # ä¿å­˜å‰ã« explanation ã‚’æ–‡å­—åˆ—ã¸å¯„ã›ã‚‹ï¼ˆä»»æ„ï¼‰
                    response_data["explanation"] = _normalize_explanation(response_data.get("explanation", ""))
                    
                    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response_data.get("answer", ""),
                        "response_data": response_data,
                        "context": context_chunks
                    })
                
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
                    error_response = {
                        "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                        "explanation": f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}",
                        "highlights": [],
                        "sources": []
                    }
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": error_response["answer"],
                        "response_data": error_response,
                        "context": []
                    })
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("ğŸ’¡ ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ç¤¾å†…è¦å®šã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™ã€‚æœ€æ–°ã®æƒ…å ±ã«ã¤ã„ã¦ã¯äººäº‹éƒ¨ã«ã”ç¢ºèªãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()